{"cells":[{"cell_type":"markdown","metadata":{},"source":["# GPT-2 Fine-Tuning"]},{"cell_type":"markdown","metadata":{},"source":["#### This is the code I wrote at the company, but I think it would be nice to share it here, so I post it.\n","\n","#### With this data, we will fine tune GPT-2 to make a sentence generation model. \n","\n","#### This code is for AI beginners."]},{"cell_type":"markdown","metadata":{},"source":["## Step 1. Data preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["#### the data contains unnecessary newlines, tags, and URLs it will be necessary to remove them before preprocessing."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import polars as pl\n","import pandas as pd\n","import numpy as np\n","import re\n","import itertools\n","from tqdm import tqdm\n","import pickle\n","import time\n","import copy"]},{"cell_type":"markdown","metadata":{},"source":["#### 読み込むデータパスの指定"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["base_path = \"/kaggle/s3storage/01_public/humob-challenge-2024/\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# cityA_df = pl.read_csv(base_path + \"input/cityA_groundtruthdata.csv.gz\")\n","cityB_df = pl.read_csv(base_path + \"input/cityB_challengedata.csv.gz\")\n","# cityC_df = pl.read_csv(base_path + \"input/cityC_challengedata.csv.gz\")\n","# cityD_df = pl.read_csv(base_path + \"input/cityD_challengedata.csv.gz\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 学習データと検証データのsplit"]},{"cell_type":"markdown","metadata":{},"source":["- 検証用のため、cityBでuid<=2000のユーザのみ抽出 "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_cityB = cityB_df.filter((pl.col('uid') < 20000) | ((pl.col('uid') >= 20000) & (pl.col('d') <= 60)))\n","valid_cityB = cityB_df.filter((pl.col('uid') >= 20000) & ((pl.col('uid') < 22000) & (pl.col('d') > 60)))"]},{"cell_type":"markdown","metadata":{},"source":["### データの前処理"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def preprocess_data(df):\n","    df = df.with_columns([\n","        pl.col(\"x\").cast(pl.Utf8).str.zfill(3).alias(\"x\"),\n","        pl.col(\"y\").cast(pl.Utf8).str.zfill(3).alias(\"y\")\n","        ])\n","    df = (\n","        df\n","        .with_columns([\n","            pl.col(\"x\").map_elements(lambda x: f\"x{x}\").alias(\"x\")\n","        ])\n","        .with_columns([\n","            pl.col(\"y\").map_elements(lambda x: f\"y{x}\").alias(\"y\")\n","        ])\n","    )\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>uid</th><th>d</th><th>t</th><th>x</th><th>y</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>20</td><td>&quot;x080&quot;</td><td>&quot;y099&quot;</td></tr><tr><td>0</td><td>0</td><td>21</td><td>&quot;x081&quot;</td><td>&quot;y097&quot;</td></tr><tr><td>0</td><td>0</td><td>25</td><td>&quot;x083&quot;</td><td>&quot;y102&quot;</td></tr><tr><td>0</td><td>0</td><td>26</td><td>&quot;x080&quot;</td><td>&quot;y101&quot;</td></tr><tr><td>0</td><td>0</td><td>27</td><td>&quot;x080&quot;</td><td>&quot;y101&quot;</td></tr></tbody></table></div>"],"text/plain":["shape: (5, 5)\n","┌─────┬─────┬─────┬──────┬──────┐\n","│ uid ┆ d   ┆ t   ┆ x    ┆ y    │\n","│ --- ┆ --- ┆ --- ┆ ---  ┆ ---  │\n","│ i64 ┆ i64 ┆ i64 ┆ str  ┆ str  │\n","╞═════╪═════╪═════╪══════╪══════╡\n","│ 0   ┆ 0   ┆ 20  ┆ x080 ┆ y099 │\n","│ 0   ┆ 0   ┆ 21  ┆ x081 ┆ y097 │\n","│ 0   ┆ 0   ┆ 25  ┆ x083 ┆ y102 │\n","│ 0   ┆ 0   ┆ 26  ┆ x080 ┆ y101 │\n","│ 0   ┆ 0   ┆ 27  ┆ x080 ┆ y101 │\n","└─────┴─────┴─────┴──────┴──────┘"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train_cityB = preprocess_data(train_cityB)\n","valid_cityB = preprocess_data(valid_cityB)\n","train_cityB.head()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_timedelta(df):\n","    \"\"\"ユーザーIDごとに時間の差分値を付与して返す\"\"\"\n","    uid_list = []\n","\n","    for uid, traj in tqdm(df.sort('uid').group_by('uid')):\n","        time_delta = np.insert((traj['d'].to_numpy()[1:] * 48 + traj['t'].to_numpy()[1:]) - (traj['d'].to_numpy()[:-1] * 48 + traj['t'].to_numpy()[:-1]), 0, 0)\n","        time_delta[time_delta > 47] = 47\n","\n","        uid_list.append(\n","            time_delta,\n","        )\n","\n","    return uid_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_delta_list = get_timedelta(train_cityB)\n","valid_delta_list = get_timedelta(valid_cityB)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_cityB.write_csv(\"train_cityB_timedelta.csv\")\n","valid_cityB# 新規列として追加\n","train_cityB = train_cityB.with_columns(pl.Series('timedelta', np.concatenate(train_delta_list)))\n","valid_cityB = valid_cityB.with_columns(pl.Series('timedelta', np.concatenate(valid_delta_list)))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 欠損値の補完\n","- 欠損しているデータは'N'として文字列補完する"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["shape: (5_760_000, 5)\n","┌─────┬─────┬──────┬──────┬──────┐\n","│ d   ┆ t   ┆ uid  ┆ x    ┆ y    │\n","│ --- ┆ --- ┆ ---  ┆ ---  ┆ ---  │\n","│ i64 ┆ i64 ┆ i64  ┆ str  ┆ str  │\n","╞═════╪═════╪══════╪══════╪══════╡\n","│ 0   ┆ 0   ┆ 0    ┆ N    ┆ N    │\n","│ 0   ┆ 1   ┆ 0    ┆ N    ┆ N    │\n","│ 0   ┆ 2   ┆ 0    ┆ N    ┆ N    │\n","│ 0   ┆ 3   ┆ 0    ┆ N    ┆ N    │\n","│ 0   ┆ 4   ┆ 0    ┆ N    ┆ N    │\n","│ …   ┆ …   ┆ …    ┆ …    ┆ …    │\n","│ 59  ┆ 43  ┆ 1999 ┆ N    ┆ N    │\n","│ 59  ┆ 44  ┆ 1999 ┆ N    ┆ N    │\n","│ 59  ┆ 45  ┆ 1999 ┆ x075 ┆ y073 │\n","│ 59  ┆ 46  ┆ 1999 ┆ x074 ┆ y074 │\n","│ 59  ┆ 47  ┆ 1999 ┆ x076 ┆ y074 │\n","└─────┴─────┴──────┴──────┴──────┘\n"]}],"source":["target_df = filtered_cityB_df\n","# 全uidのリストを取得\n","uids = target_df['uid'].unique()\n","\n","# d: 0-74, t: 0-47 の全組み合わせを作成\n","all_days = list(range(60))\n","all_times = list(range(48))\n","all_combinations = list(itertools.product(all_days, all_times))\n","\n","# 全uidに対して欠損している組み合わせを補完\n","full_data = []\n","\n","for uid in uids:\n","    # 既存のデータをフィルタリング\n","    existing_data = target_df.filter(pl.col('uid') == uid)\n","    \n","    # 全組み合わせをDataFrameに変換\n","    full_combinations = pl.DataFrame(all_combinations, schema=['d', 't'])\n","    full_combinations = full_combinations.with_columns([\n","        pl.lit(uid).cast(pl.Int64).alias('uid')  # キャストを追加\n","    ])\n","    \n","    # 既存データと全組み合わせをマージして、欠損部分を見つける\n","    merged_data = full_combinations.join(existing_data, on=['uid', 'd', 't'], how='left')\n","    \n","    # x, y の欠損を 'N' で埋める\n","    merged_data = merged_data.fill_null('N')\n","    \n","    # 結果を追加\n","    full_data.append(merged_data)\n","\n","# 結果を結合\n","full_df = pl.concat(full_data)\n","\n","# 結果を保存\n","# full_data.write_csv('full_data.csv')\n","\n","# 結果を確認\n","print(full_df)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>d</th><th>t</th><th>uid</th><th>x</th><th>y</th><th>xy</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>59</td><td>43</td><td>1999</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot;NN&quot;</td></tr><tr><td>59</td><td>44</td><td>1999</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot;NN&quot;</td></tr><tr><td>59</td><td>45</td><td>1999</td><td>&quot;x075&quot;</td><td>&quot;y073&quot;</td><td>&quot;x075y073&quot;</td></tr><tr><td>59</td><td>46</td><td>1999</td><td>&quot;x074&quot;</td><td>&quot;y074&quot;</td><td>&quot;x074y074&quot;</td></tr><tr><td>59</td><td>47</td><td>1999</td><td>&quot;x076&quot;</td><td>&quot;y074&quot;</td><td>&quot;x076y074&quot;</td></tr></tbody></table></div>"],"text/plain":["shape: (5, 6)\n","┌─────┬─────┬──────┬──────┬──────┬──────────┐\n","│ d   ┆ t   ┆ uid  ┆ x    ┆ y    ┆ xy       │\n","│ --- ┆ --- ┆ ---  ┆ ---  ┆ ---  ┆ ---      │\n","│ i64 ┆ i64 ┆ i64  ┆ str  ┆ str  ┆ str      │\n","╞═════╪═════╪══════╪══════╪══════╪══════════╡\n","│ 59  ┆ 43  ┆ 1999 ┆ N    ┆ N    ┆ NN       │\n","│ 59  ┆ 44  ┆ 1999 ┆ N    ┆ N    ┆ NN       │\n","│ 59  ┆ 45  ┆ 1999 ┆ x075 ┆ y073 ┆ x075y073 │\n","│ 59  ┆ 46  ┆ 1999 ┆ x074 ┆ y074 ┆ x074y074 │\n","│ 59  ┆ 47  ┆ 1999 ┆ x076 ┆ y074 ┆ x076y074 │\n","└─────┴─────┴──────┴──────┴──────┴──────────┘"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["full_df = (\n","    full_df.with_columns([\n","        pl.concat_str([pl.col(\"x\"), pl.col(\"y\")]).alias(\"xy\")\n","    ])\n",")\n","full_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#[\"x01y02,x02y03,,,,,|13,2,4......|\"]\n","#[\"x01y02_1_,x02y03,,,,,|13,2,4......|\"]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>uid</th><th>d</th><th>xy</th></tr><tr><td>i64</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>1999</td><td>55</td><td>[&quot;NN&quot;, &quot;NN&quot;, … &quot;NN&quot;]</td></tr><tr><td>1999</td><td>56</td><td>[&quot;NN&quot;, &quot;NN&quot;, … &quot;x075y073&quot;]</td></tr><tr><td>1999</td><td>57</td><td>[&quot;x074y074&quot;, &quot;NN&quot;, … &quot;x075y073&quot;]</td></tr><tr><td>1999</td><td>58</td><td>[&quot;NN&quot;, &quot;NN&quot;, … &quot;x075y073&quot;]</td></tr><tr><td>1999</td><td>59</td><td>[&quot;NN&quot;, &quot;NN&quot;, … &quot;x076y074&quot;]</td></tr></tbody></table></div>"],"text/plain":["shape: (5, 3)\n","┌──────┬─────┬──────────────────────────────────┐\n","│ uid  ┆ d   ┆ xy                               │\n","│ ---  ┆ --- ┆ ---                              │\n","│ i64  ┆ i64 ┆ list[str]                        │\n","╞══════╪═════╪══════════════════════════════════╡\n","│ 1999 ┆ 55  ┆ [\"NN\", \"NN\", … \"NN\"]             │\n","│ 1999 ┆ 56  ┆ [\"NN\", \"NN\", … \"x075y073\"]       │\n","│ 1999 ┆ 57  ┆ [\"x074y074\", \"NN\", … \"x075y073\"] │\n","│ 1999 ┆ 58  ┆ [\"NN\", \"NN\", … \"x075y073\"]       │\n","│ 1999 ┆ 59  ┆ [\"NN\", \"NN\", … \"x076y074\"]       │\n","└──────┴─────┴──────────────────────────────────┘"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# 日付ごとにValue値を結合\n","df_aggregated = full_df.group_by('uid', 'd').agg([\n","    pl.concat_str(\"xy\", separator=\",\").alias('xy')\n","]).sort(\"uid\", \"d\")\n","df_aggregated.tail()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2. Model Training\n","GPT2モデルによるFine-Tunningを実施する。  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Trainerを使用した基本的な実装例\n","\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# カスタムデータクラス\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_dataset(file_path, tokenizer, block_size = 128):\n","    dataset = TextDataset(\n","        tokenizer = tokenizer,\n","        file_path = file_path,\n","        block_size = block_size,\n","    )\n","    return dataset\n","\n","\n","def load_data_collator(tokenizer, mlm = False):\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, \n","        mlm=mlm,\n","    )\n","    return data_collator\n","\n","\n","def train(train_file_path,model_name,\n","          output_dir,\n","          overwrite_output_dir,\n","          per_device_train_batch_size,\n","          num_train_epochs,\n","          save_steps):\n","  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","  train_dataset = load_dataset(train_file_path, tokenizer)\n","  data_collator = load_data_collator(tokenizer)\n","\n","  tokenizer.save_pretrained(output_dir)\n","      \n","  model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","  model.save_pretrained(output_dir)\n","\n","  training_args = TrainingArguments(\n","          output_dir=output_dir,\n","          overwrite_output_dir=overwrite_output_dir,\n","          per_device_train_batch_size=per_device_train_batch_size,\n","          num_train_epochs=num_train_epochs,\n","      )\n","\n","  trainer = Trainer(\n","          model=model,\n","          args=training_args,\n","          data_collator=data_collator,\n","          train_dataset=train_dataset,\n","  )\n","      \n","  trainer.train()\n","  trainer.save_model()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# you need to set parameters \n","train_file_path = \"/content/drive/MyDrive/Articles.txt\"\n","model_name = 'gpt2'\n","output_dir = '/content/drive/MyDrive/result'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 5.0\n","save_steps = 500"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# It takes about 30 minutes to train in colab.\n","train(\n","    train_file_path=train_file_path,\n","    model_name=model_name,\n","    output_dir=output_dir,\n","    overwrite_output_dir=overwrite_output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    num_train_epochs=num_train_epochs,\n","    save_steps=save_steps\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3. Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model(model_path):\n","    model = GPT2LMHeadModel.from_pretrained(model_path)\n","    return model\n","\n","\n","def load_tokenizer(tokenizer_path):\n","    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n","    return tokenizer\n","\n","\n","def generate_text(sequence, max_length):\n","    model_path = \"/content/drive/MyDrive/result\"\n","    model = load_model(model_path)\n","    tokenizer = load_tokenizer(model_path)\n","    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n","    final_outputs = model.generate(\n","        ids,\n","        do_sample=True,\n","        max_length=max_length,\n","        pad_token_id=model.config.eos_token_id,\n","        top_k=50,\n","        top_p=0.95,\n","    )\n","    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequence = input() # oil price\n","max_len = int(input()) # 20\n","generate_text(sequence, max_len) # oil price for July June which had been low at as low as was originally stated Prices have since resumed"]},{"cell_type":"markdown","metadata":{},"source":["The following process may be a little more complicated or tedious because you have to write the code one by one, and it takes a long time if you don't have a personal GPU.\n","\n","Then, how about use Ainize's Teachable NLP? Teachable NLP provides an API to use the model so when data is input it will automatically learn quickly.\n","\n","Teachable NLP : [https://ainize.ai/teachable-nlp](https://link.ainize.ai/3tJVRD1)\n","\n","Teachable NLP Tutorial : [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://link.ainize.ai/3tATaUh)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
